{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ParallelTransformerEmbedding.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1KK2BkaziREsPQ5QdqnEmiv00FDT548Ff","authorship_tag":"ABX9TyMoUNwFL2/VLA7Ly4omWjvx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"oOfDhL82vAja","colab_type":"text"},"source":["References: <br>\n","https://github.com/bentrevett/pytorch-seq2seq <br>\n","https://pytorch.org/tutorials/beginner/data_loading_tutorial.html<br>\n","Source Directory: https://www.statmt.org/europarl/ <br>\n","Data directory: https://drive.google.com/drive/folders/1Wwz0WPKWuc1RimMLze9klzAdRS-nGNw1?usp=sharing <br>\n","Please have all the files in the same folder for the code to execute"]},{"cell_type":"markdown","metadata":{"id":"dJGUPdpqtaik","colab_type":"text"},"source":["Importing the libraries: We use Pytorch to train our models"]},{"cell_type":"code","metadata":{"id":"QywbUwfFbzag","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","import torchtext\n","from torchtext.data import TabularDataset\n","from torchtext.data import Field, BucketIterator\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","\n","from spacy.lang.en import English\n","from spacy.lang.fr import French\n","from spacy.lang.es import Spanish\n","import numpy as np\n","import torch.nn.functional as F\n","import random\n","import math\n","import time\n","import pandas as pd\n","from nltk.translate.bleu_score import sentence_bleu"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cPIpkkxttejf","colab_type":"text"},"source":["Randomize the parameters for pytorch"]},{"cell_type":"code","metadata":{"id":"J3pjIpYwb_fT","colab_type":"code","colab":{}},"source":["SEED = 1234\n","\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0zU3lA1gfbr9","colab_type":"code","colab":{}},"source":["device = 'cuda'"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NOyYUInttmS6","colab_type":"text"},"source":["Loading the tokenizer and tokenizing the sentences <br>\n","Creating field values for the source, target1, target2 <br>\n","target1 represents spanish <br>\n","target2 represents French"]},{"cell_type":"code","metadata":{"id":"CVDX8gQhfGJx","colab_type":"code","colab":{}},"source":["spacy_en = English()\n","spacy_es = Spanish()\n","spacy_fr = French()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EUg2CTFefIV6","colab_type":"code","colab":{}},"source":["def tokenize_en(text):\n","    return [tok.text for tok in spacy_en.tokenizer(text)]\n","\n","def tokenize_es(text):\n","    return [tok.text for tok in spacy_es.tokenizer(text)]\n","\n","def tokenize_fr(text):\n","    return [tok.text for tok in spacy_fr.tokenizer(text)]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2sED0ZEwfKnC","colab_type":"code","colab":{}},"source":["SRC = Field(tokenize = tokenize_en, \n","            init_token = '<sos>', \n","            eos_token = '<eos>', \n","            lower = True, \n","            batch_first = True)\n","\n","TRG1 = Field(tokenize = tokenize_es, \n","            init_token = '<sos>', \n","            eos_token = '<eos>', \n","            lower = True, \n","            batch_first = True)\n","\n","TRG2 = Field(tokenize = tokenize_fr, \n","            init_token = '<sos>', \n","            eos_token = '<eos>', \n","            lower = True, \n","            batch_first = True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3n4uHyAMfN0s","colab_type":"code","colab":{}},"source":["tv_datafields = [(\"English\", SRC),\n","                 (\"Spanish\", TRG1),\n","                 (\"French\", TRG2)]\n","trn, vld, tst = TabularDataset.splits(\n","               path = \"./\",\n","               train='train_df.csv', validation=\"val_df.csv\", test=\"test_df.csv\",\n","               format='csv',\n","               skip_header=True,\n","               fields=tv_datafields)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IekvNbDXtsBG","colab_type":"text"},"source":["Building the vocabulary and creating batches for the model. <br>\n","We consider a batch size of 50"]},{"cell_type":"code","metadata":{"id":"oEx49DTcfQXH","colab_type":"code","colab":{}},"source":["SRC.build_vocab(trn, min_freq = 2)\n","TRG1.build_vocab(trn, min_freq = 2)\n","TRG2.build_vocab(trn, min_freq = 2)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"c1RKXNjefeMM","colab_type":"code","colab":{}},"source":["BATCH_SIZE = 50\n","train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n","    (trn, vld, tst),\n","     sort_key=lambda x: len(x.English),\n","     sort_within_batch=False,\n","     batch_size = BATCH_SIZE,\n","     device = device, \n","     repeat=False)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3_HMRIsLtxdI","colab_type":"text"},"source":["Loading the pretrained GLOVE word embedding model and converting our vocab to 300 dimension weights"]},{"cell_type":"code","metadata":{"id":"BmNcX52Lfggb","colab_type":"code","colab":{}},"source":["word_embeddings = {}\n","f = open('glove.6B.300d.txt', encoding='utf-8')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HxUzC6rOn-Q7","colab_type":"code","colab":{}},"source":["for line in f:\n","  values = line.split()\n","  word = values[0]\n","  coefs = np.asarray(values[1:], dtype='float32')\n","  word_embeddings[word] = coefs"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uGuq7Cl9oFeN","colab_type":"code","colab":{}},"source":["INPUT_DIM = len(SRC.vocab) \n","weight_encoder = np.zeros((INPUT_DIM, 300))\n","words_found = 0\n","\n","for i, word in enumerate(SRC.vocab.itos):\n","    try: \n","        weight_encoder[i] = word_embeddings[word]\n","        words_found += 1\n","    except KeyError:\n","        weight_encoder[i] = word_embeddings['unk']\n","\n","INPUT_DIM_DEC1 = len(TRG1.vocab) \n","weight_decoder1 = np.zeros((INPUT_DIM_DEC1, 300))\n","words_found = 0\n","\n","for i, word in enumerate(TRG1.vocab.itos):\n","    try: \n","        weight_decoder1[i] = word_embeddings[word]\n","        words_found += 1\n","    except KeyError:\n","        weight_decoder1[i] = word_embeddings['unk']\n","\n","INPUT_DIM_DEC2 = len(TRG2.vocab) \n","weight_decoder2 = np.zeros((INPUT_DIM_DEC2, 300))\n","words_found = 0\n","\n","for i, word in enumerate(TRG2.vocab.itos):\n","    try: \n","        weight_decoder1[i] = word_embeddings[word]\n","        words_found += 1\n","    except KeyError:\n","        weight_decoder1[i] = word_embeddings['unk']"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nQfMuJeatzjg","colab_type":"text"},"source":["Design of the network module:<br>\n","1) Encoder: We initialize the embedding layer with pre-trained word embedding for source<br>\n","2) EncoderLayer <br>\n","3) MultiHeadedAttentionLayer <br>\n","4) PositionWiseEncoder <br> \n","5) Decoder: We pass the weight matrix as an argument and initialize the embedding for target <br>\n","6) DecoderLayer<br>"]},{"cell_type":"code","metadata":{"id":"hzbw8cnko3Z8","colab_type":"code","colab":{}},"source":["class Encoder(nn.Module):\n","    def __init__(self, \n","                 input_dim, \n","                 hid_dim, \n","                 n_layers, \n","                 n_heads, \n","                 pf_dim,\n","                 dropout, \n","                 device,\n","                 max_length = 400):\n","        super().__init__()\n","\n","        self.device = device\n","        \n","        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n","        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n","        #Pretrained Word Embedding\n","        emb_weights = torch.Tensor(weight_encoder)\n","        self.tok_embedding.weight = nn.Parameter(emb_weights)\n","        self.tok_embedding.weight.requires_grad = False\n","\n","        self.layers = nn.ModuleList([EncoderLayer(hid_dim, \n","                                                  n_heads, \n","                                                  pf_dim,\n","                                                  dropout, \n","                                                  device) \n","                                     for _ in range(n_layers)])\n","        \n","        self.dropout = nn.Dropout(dropout)\n","        \n","        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n","        \n","    def forward(self, src, src_mask):\n","        \n","        #src = [batch size, src len]\n","        #src_mask = [batch size, src len]\n","        \n","        batch_size = src.shape[0]\n","        src_len = src.shape[1]\n","        \n","        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n","        \n","        #pos = [batch size, src len]\n","        \n","        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n","        \n","        #src = [batch size, src len, hid dim]\n","        \n","        for layer in self.layers:\n","            src = layer(src, src_mask)\n","            \n","        #src = [batch size, src len, hid dim]\n","            \n","        return src"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TYDfeXcKtEe4","colab_type":"code","colab":{}},"source":["class EncoderLayer(nn.Module):\n","    def __init__(self, \n","                 hid_dim, \n","                 n_heads, \n","                 pf_dim,  \n","                 dropout, \n","                 device):\n","        super().__init__()\n","        \n","        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n","        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n","        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n","        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n","                                                                     pf_dim, \n","                                                                     dropout)\n","        self.dropout = nn.Dropout(dropout)\n","        \n","    def forward(self, src, src_mask):\n","        \n","        #src = [batch size, src len, hid dim]\n","        #src_mask = [batch size, src len]\n","                \n","        #self attention\n","        _src, _ = self.self_attention(src, src, src, src_mask)\n","        \n","        #dropout, residual connection and layer norm\n","        src = self.self_attn_layer_norm(src + self.dropout(_src))\n","        \n","        #src = [batch size, src len, hid dim]\n","        \n","        #positionwise feedforward\n","        _src = self.positionwise_feedforward(src)\n","        \n","        #dropout, residual and layer norm\n","        src = self.ff_layer_norm(src + self.dropout(_src))\n","        \n","        #src = [batch size, src len, hid dim]\n","        \n","        return src"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IV6d1-cjtL-U","colab_type":"code","colab":{}},"source":["class MultiHeadAttentionLayer(nn.Module):\n","    def __init__(self, hid_dim, n_heads, dropout, device):\n","        super().__init__()\n","        \n","        assert hid_dim % n_heads == 0\n","        \n","        self.hid_dim = hid_dim\n","        self.n_heads = n_heads\n","        self.head_dim = hid_dim // n_heads\n","        \n","        self.fc_q = nn.Linear(hid_dim, hid_dim)\n","        self.fc_k = nn.Linear(hid_dim, hid_dim)\n","        self.fc_v = nn.Linear(hid_dim, hid_dim)\n","        \n","        self.fc_o = nn.Linear(hid_dim, hid_dim)\n","        \n","        self.dropout = nn.Dropout(dropout)\n","        \n","        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n","        \n","    def forward(self, query, key, value, mask = None):\n","        \n","        batch_size = query.shape[0]\n","        \n","        #query = [batch size, query len, hid dim]\n","        #key = [batch size, key len, hid dim]\n","        #value = [batch size, value len, hid dim]\n","                \n","        Q = self.fc_q(query)\n","        K = self.fc_k(key)\n","        V = self.fc_v(value)\n","        \n","        #Q = [batch size, query len, hid dim]\n","        #K = [batch size, key len, hid dim]\n","        #V = [batch size, value len, hid dim]\n","                \n","        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n","        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n","        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n","        \n","        #Q = [batch size, n heads, query len, head dim]\n","        #K = [batch size, n heads, key len, head dim]\n","        #V = [batch size, n heads, value len, head dim]\n","                \n","        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n","        \n","        #energy = [batch size, n heads, query len, key len]\n","        \n","        if mask is not None:\n","            energy = energy.masked_fill(mask == 0, -1e10)\n","        \n","        attention = torch.softmax(energy, dim = -1)\n","                \n","        #attention = [batch size, n heads, query len, key len]\n","                \n","        x = torch.matmul(self.dropout(attention), V)\n","        \n","        #x = [batch size, n heads, query len, head dim]\n","        \n","        x = x.permute(0, 2, 1, 3).contiguous()\n","        \n","        #x = [batch size, query len, n heads, head dim]\n","        \n","        x = x.view(batch_size, -1, self.hid_dim)\n","        \n","        #x = [batch size, query len, hid dim]\n","        \n","        x = self.fc_o(x)\n","        \n","        #x = [batch size, query len, hid dim]\n","        \n","        return x, attention"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TXI1lC13tQru","colab_type":"code","colab":{}},"source":["class PositionwiseFeedforwardLayer(nn.Module):\n","    def __init__(self, hid_dim, pf_dim, dropout):\n","        super().__init__()\n","        \n","        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n","        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n","        \n","        self.dropout = nn.Dropout(dropout)\n","        \n","    def forward(self, x):\n","        \n","        #x = [batch size, seq len, hid dim]\n","        \n","        x = self.dropout(torch.relu(self.fc_1(x)))\n","        \n","        #x = [batch size, seq len, pf dim]\n","        \n","        x = self.fc_2(x)\n","        \n","        #x = [batch size, seq len, hid dim]\n","        \n","        return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2a2ELC4FtSm6","colab_type":"code","colab":{}},"source":["class Decoder(nn.Module):\n","    def __init__(self, \n","                 output_dim, \n","                 hid_dim, \n","                 n_layers, \n","                 n_heads, \n","                 pf_dim, \n","                 dropout, \n","                 device,\n","                 weight_matrix,\n","                 max_length = 400):\n","        super().__init__()\n","        \n","        self.device = device\n","        \n","        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n","        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n","        \n","        #Pretrained Word Embedding\n","        #Pretrained Word Embedding\n","        emb_weights = torch.Tensor(weight_matrix)\n","        self.tok_embedding.weight = nn.Parameter(emb_weights)\n","        self.tok_embedding.weight.requires_grad = False\n","        self.layers = nn.ModuleList([DecoderLayer(hid_dim, \n","                                                  n_heads, \n","                                                  pf_dim, \n","                                                  dropout, \n","                                                  device)\n","                                     for _ in range(n_layers)])\n","        \n","        self.fc_out = nn.Linear(hid_dim, output_dim)\n","        \n","        self.dropout = nn.Dropout(dropout)\n","        \n","        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n","        \n","    def forward(self, trg, enc_src, trg_mask, src_mask):\n","        \n","        #trg = [batch size, trg len]\n","        #enc_src = [batch size, src len, hid dim]\n","        #trg_mask = [batch size, trg len]\n","        #src_mask = [batch size, src len]\n","                \n","        batch_size = trg.shape[0]\n","        trg_len = trg.shape[1]\n","        \n","        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n","                       \n","        #pos = [batch size, trg len]\n","            \n","        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n","                \n","        #trg = [batch size, trg len, hid dim]\n","        \n","        for layer in self.layers:\n","            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n","        \n","        #trg = [batch size, trg len, hid dim]\n","        #attention = [batch size, n heads, trg len, src len]\n","        \n","        output = self.fc_out(trg)\n","        \n","        #output = [batch size, trg len, output dim]\n","            \n","        return output, attention"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ACVtx6iZtfX6","colab_type":"code","colab":{}},"source":["class DecoderLayer(nn.Module):\n","    def __init__(self, \n","                 hid_dim, \n","                 n_heads, \n","                 pf_dim, \n","                 dropout, \n","                 device):\n","        super().__init__()\n","        \n","        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n","        self.enc_attn_layer_norm = nn.LayerNorm(hid_dim)\n","        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n","        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n","        self.encoder_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n","        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n","                                                                     pf_dim, \n","                                                                     dropout)\n","        self.dropout = nn.Dropout(dropout)\n","        \n","    def forward(self, trg, enc_src, trg_mask, src_mask):\n","        \n","        #trg = [batch size, trg len, hid dim]\n","        #enc_src = [batch size, src len, hid dim]\n","        #trg_mask = [batch size, trg len]\n","        #src_mask = [batch size, src len]\n","        \n","        #self attention\n","        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n","        \n","        #dropout, residual connection and layer norm\n","        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n","            \n","        #trg = [batch size, trg len, hid dim]\n","            \n","        #encoder attention\n","        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n","        \n","        #dropout, residual connection and layer norm\n","        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n","                    \n","        #trg = [batch size, trg len, hid dim]\n","        \n","        #positionwise feedforward\n","        _trg = self.positionwise_feedforward(trg)\n","        \n","        #dropout, residual and layer norm\n","        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n","        \n","        #trg = [batch size, trg len, hid dim]\n","        #attention = [batch size, n heads, trg len, src len]\n","        \n","        return trg, attention"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DqKra3DBuV3C","colab_type":"text"},"source":["Seq2Seq module <br>\n","Here we specify the sequence to sequence module flow. <br>\n","The encoded source goes into both the decoders, decoder1 and decoder2"]},{"cell_type":"code","metadata":{"id":"mQcByw8qtza2","colab_type":"code","colab":{}},"source":["class Seq2Seq(nn.Module):\n","    def __init__(self, \n","                 encoder, \n","                 decoder1,\n","                 decoder2, \n","                 src_pad_idx, \n","                 trg1_pad_idx,\n","                 trg2_pad_idx,\n","                 device):\n","        super().__init__()\n","        \n","        self.encoder = encoder\n","        self.decoder1 = decoder1\n","        self.decoder2 = decoder2\n","        self.src_pad_idx = src_pad_idx\n","        self.trg1_pad_idx = trg1_pad_idx\n","        self.trg2_pad_idx = trg2_pad_idx,\n","        self.device = device\n","        \n","    def make_src_mask(self, src):\n","        \n","        #src = [batch size, src len]\n","        \n","        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n","\n","        #src_mask = [batch size, 1, 1, src len]\n","\n","        return src_mask\n","    \n","    def make_trg_mask(self, trg):\n","        \n","        #trg = [batch size, trg len]\n","        \n","        trg_pad_mask = (trg != self.trg1_pad_idx).unsqueeze(1).unsqueeze(2)\n","        \n","        #trg_pad_mask = [batch size, 1, 1, trg len]\n","        \n","        trg_len = trg.shape[1]\n","        \n","        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n","        \n","        #trg_sub_mask = [trg len, trg len]\n","            \n","        trg_mask = trg_pad_mask & trg_sub_mask\n","        \n","        #trg_mask = [batch size, 1, trg len, trg len]\n","        \n","        return trg_mask\n","\n","    def forward(self, src, trg1, trg2):\n","        \n","        #src = [batch size, src len]\n","        #trg = [batch size, trg len]\n","                \n","        src_mask = self.make_src_mask(src)\n","        trg_mask1 = self.make_trg_mask(trg1)\n","        trg_mask2 = self.make_trg_mask(trg2)\n","        \n","        #src_mask = [batch size, 1, 1, src len]\n","        #trg_mask = [batch size, 1, trg len, trg len]\n","        \n","        enc_src = self.encoder(src, src_mask)\n","\n","        #enc_src = [batch size, src len, hid dim]\n","        output1, attention1 = self.decoder1(trg1, enc_src, trg_mask1, src_mask)\n","        output2, attention2 = self.decoder2(trg2, enc_src, trg_mask2, src_mask)\n","        \n","        #output = [batch size, trg len, output dim]\n","        #attention = [batch size, n heads, trg len, src len]\n","        \n","        return output1, output2"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-I5yPYAcuY_L","colab_type":"text"},"source":["Network Parameters<br>\n","Module block creation<br>"]},{"cell_type":"code","metadata":{"id":"v01SYzjFt1UU","colab_type":"code","colab":{}},"source":["INPUT_DIM = len(SRC.vocab)\n","OUTPUT_DIM1 = len(TRG1.vocab)\n","OUTPUT_DIM2 = len(TRG2.vocab)\n","HID_DIM = 300\n","ENC_LAYERS = 1\n","DEC_LAYERS = 1\n","ENC_HEADS = 10\n","DEC_HEADS = 10\n","ENC_PF_DIM = 256\n","DEC_PF_DIM = 256\n","ENC_DROPOUT = 0.1\n","DEC_DROPOUT = 0.1\n","\n","enc = Encoder(INPUT_DIM, \n","              HID_DIM, \n","              ENC_LAYERS, \n","              ENC_HEADS, \n","              ENC_PF_DIM, \n","              ENC_DROPOUT, \n","              device)\n","\n","dec1 = Decoder(OUTPUT_DIM1, \n","              HID_DIM, \n","              DEC_LAYERS, \n","              DEC_HEADS, \n","              DEC_PF_DIM, \n","              DEC_DROPOUT, \n","              device,\n","              weight_decoder1)\n","\n","dec2 = Decoder(OUTPUT_DIM2,\n","               HID_DIM,\n","               DEC_LAYERS,\n","               DEC_HEADS,\n","               DEC_PF_DIM,\n","               DEC_DROPOUT,\n","               device,\n","               weight_decoder2)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PcNuzzqtucWQ","colab_type":"text"},"source":["SRC, TRG1, TRG2 vocab creation <br>\n","Initialize weights <br>\n","Optimizer: Adam optimizer<br>\n","Criterion: Cross Entropy<br>"]},{"cell_type":"code","metadata":{"id":"ePXIKGZsuVjr","colab_type":"code","colab":{}},"source":["SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]SRC, TRG1, TRG2 vocab creation <br>\n","Initialize weights <br>\n","Optimizer: Adam optimizer<br>\n","Criterion: Cross Entropy<br>\n","TRG1_PAD_IDX = TRG1.vocab.stoi[TRG1.pad_token]\n","TRG2_PAD_IDX = TRG2.vocab.stoi[TRG2.pad_token]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yh3i4h0-0W8V","colab_type":"code","colab":{}},"source":["model = Seq2Seq(enc, dec1, dec2, SRC_PAD_IDX, TRG1_PAD_IDX, TRG2_PAD_IDX, device).to(device)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4oY0SDdx0YlL","colab_type":"code","colab":{}},"source":["def initialize_weights(m):\n","    if hasattr(m, 'weight') and m.weight.dim() > 1:\n","        nn.init.xavier_uniform_(m.weight.data)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"L9A5LXWF0b0O","colab_type":"code","colab":{}},"source":["model.apply(initialize_weights);"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rowhHMFS0di9","colab_type":"code","colab":{}},"source":["LEARNING_RATE = 0.001\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tiRWKKFm0fc3","colab_type":"code","colab":{}},"source":["criterion = nn.CrossEntropyLoss(ignore_index = TRG1_PAD_IDX)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oKs8WJCLugwl","colab_type":"text"},"source":["The main idea behind this parallel model is derived from the loss propagation. <br>\n","In this case, we can have the loss being propagated from the network via two parallel paths, and hence we need to provide an additonal parameter saying retain_graph as true for the first loss we propagate. <br>"]},{"cell_type":"code","metadata":{"id":"1sp0F1Qd0hfl","colab_type":"code","colab":{}},"source":["def train(model, iterator, optimizer, criterion, clip):\n","    \n","    model.train()\n","    \n","    epoch_loss1 = 0\n","    epoch_loss2 = 0\n","    if len(iterator)==0:\n","      return None\n","    for i, batch in enumerate(iterator):\n","        #print(\"Batch no: \", i)\n","        loss1 = 0\n","        loss2 = 0\n","        src = batch.English\n","        trg1 = batch.Spanish\n","        trg2 = batch.French\n","        \n","        optimizer.zero_grad()\n","        \n","        output1, output2 = model(src, trg1[:,:-1], trg2[:,:-1])\n","                \n","        #output = [batch size, trg len - 1, output dim]\n","        #trg = [batch size, trg len]\n","            \n","        output_dim1 = output1.shape[-1]\n","        output_dim2 = output2.shape[-1]\n","\n","        output1 = output1.contiguous().view(-1, output_dim1)\n","        output2 = output2.contiguous().view(-1, output_dim2)\n","\n","        trg1 = trg1[:,1:].contiguous().view(-1)\n","        trg2 = trg2[:,1:].contiguous().view(-1)\n","                \n","        #output = [batch size * trg len - 1, output dim]\n","        #trg = [batch size * trg len - 1]\n","           \n","        loss1 = criterion(output1, trg1)\n","        loss2 = criterion(output2, trg2)\n","\n","        loss1.backward(retain_graph=True)\n","        loss2.backward()\n","\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n","        \n","        optimizer.step()\n","        \n","        epoch_loss1 += float(loss1)\n","        epoch_loss2 += float(loss2)\n","        \n","    return epoch_loss1 / len(iterator), epoch_loss2 / len(iterator)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iRnKf-Kw0jF3","colab_type":"code","colab":{}},"source":["def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IaAIS71Gum1-","colab_type":"text"},"source":["Training loop"]},{"cell_type":"code","metadata":{"id":"yO3-chcI0miS","colab_type":"code","outputId":"9e29bfe3-0de1-41aa-f767-fb32591cdc1d","executionInfo":{"status":"ok","timestamp":1591905970029,"user_tz":420,"elapsed":2071742,"user":{"displayName":"Anurag Sengupta","photoUrl":"","userId":"05018465714817613204"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["N_EPOCHS = 20\n","CLIP = 1\n","\n","best_valid_loss = float('inf')\n","\n","for epoch in range(N_EPOCHS):\n","    \n","    start_time = time.time()\n","    \n","    train_loss1, train_loss2 = train(model, train_iterator, optimizer, criterion, CLIP)\n","    \n","    end_time = time.time()\n","    \n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","    \n","    #if valid_loss < best_valid_loss:\n","        #best_valid_loss = valid_loss\n","        #torch.save(model.state_dict(), 'tut6-model.pt')\n","    \n","    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss1: {train_loss1:.3f} | Train PPL: {math.exp(train_loss1):7.3f}')\n","    print(f'\\tTrain Loss2: {train_loss2:.3f} | Train PPL: {math.exp(train_loss2):7.3f}')\n","torch.save(model.state_dict(), 'transformer-embedded-model.pt')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch: 01 | Time: 1m 43s\n","\tTrain Loss1: 4.718 | Train PPL: 111.970\n","\tTrain Loss2: 4.682 | Train PPL: 107.935\n","Epoch: 02 | Time: 1m 43s\n","\tTrain Loss1: 3.588 | Train PPL:  36.145\n","\tTrain Loss2: 3.576 | Train PPL:  35.728\n","Epoch: 03 | Time: 1m 43s\n","\tTrain Loss1: 3.134 | Train PPL:  22.968\n","\tTrain Loss2: 3.152 | Train PPL:  23.385\n","Epoch: 04 | Time: 1m 43s\n","\tTrain Loss1: 2.862 | Train PPL:  17.493\n","\tTrain Loss2: 2.897 | Train PPL:  18.112\n","Epoch: 05 | Time: 1m 43s\n","\tTrain Loss1: 2.670 | Train PPL:  14.435\n","\tTrain Loss2: 2.718 | Train PPL:  15.146\n","Epoch: 06 | Time: 1m 43s\n","\tTrain Loss1: 2.533 | Train PPL:  12.590\n","\tTrain Loss2: 2.591 | Train PPL:  13.340\n","Epoch: 07 | Time: 1m 42s\n","\tTrain Loss1: 2.432 | Train PPL:  11.377\n","\tTrain Loss2: 2.494 | Train PPL:  12.105\n","Epoch: 08 | Time: 1m 42s\n","\tTrain Loss1: 2.353 | Train PPL:  10.513\n","\tTrain Loss2: 2.418 | Train PPL:  11.229\n","Epoch: 09 | Time: 1m 43s\n","\tTrain Loss1: 2.285 | Train PPL:   9.826\n","\tTrain Loss2: 2.356 | Train PPL:  10.548\n","Epoch: 10 | Time: 1m 43s\n","\tTrain Loss1: 2.227 | Train PPL:   9.272\n","\tTrain Loss2: 2.301 | Train PPL:   9.982\n","Epoch: 11 | Time: 1m 44s\n","\tTrain Loss1: 2.178 | Train PPL:   8.831\n","\tTrain Loss2: 2.255 | Train PPL:   9.535\n","Epoch: 12 | Time: 1m 44s\n","\tTrain Loss1: 2.137 | Train PPL:   8.472\n","\tTrain Loss2: 2.215 | Train PPL:   9.162\n","Epoch: 13 | Time: 1m 43s\n","\tTrain Loss1: 2.100 | Train PPL:   8.167\n","\tTrain Loss2: 2.181 | Train PPL:   8.856\n","Epoch: 14 | Time: 1m 43s\n","\tTrain Loss1: 2.070 | Train PPL:   7.927\n","\tTrain Loss2: 2.152 | Train PPL:   8.602\n","Epoch: 15 | Time: 1m 43s\n","\tTrain Loss1: 2.041 | Train PPL:   7.699\n","\tTrain Loss2: 2.126 | Train PPL:   8.377\n","Epoch: 16 | Time: 1m 43s\n","\tTrain Loss1: 2.018 | Train PPL:   7.521\n","\tTrain Loss2: 2.102 | Train PPL:   8.183\n","Epoch: 17 | Time: 1m 43s\n","\tTrain Loss1: 1.995 | Train PPL:   7.351\n","\tTrain Loss2: 2.081 | Train PPL:   8.012\n","Epoch: 18 | Time: 1m 43s\n","\tTrain Loss1: 1.975 | Train PPL:   7.209\n","\tTrain Loss2: 2.062 | Train PPL:   7.859\n","Epoch: 19 | Time: 1m 42s\n","\tTrain Loss1: 1.956 | Train PPL:   7.069\n","\tTrain Loss2: 2.045 | Train PPL:   7.726\n","Epoch: 20 | Time: 1m 43s\n","\tTrain Loss1: 1.942 | Train PPL:   6.970\n","\tTrain Loss2: 2.029 | Train PPL:   7.609\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FXtOxHJN0otD","colab_type":"code","colab":{}},"source":["model_loaded = model.load_state_dict(torch.load('transformer-embedded-model.pt'))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1Zrj4yNu_oEt","colab_type":"code","outputId":"d26d0956-6d99-451e-e8b3-a358f48c9074","executionInfo":{"status":"ok","timestamp":1591996300292,"user_tz":420,"elapsed":510,"user":{"displayName":"Anurag Sengupta","photoUrl":"","userId":"05018465714817613204"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["model_loaded"],"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"markdown","metadata":{"id":"2EXXrUbLuqaF","colab_type":"text"},"source":["The inference model, for translating the sentences. <br>\n","This is similar to the seq2seq module except the back propagation"]},{"cell_type":"code","metadata":{"id":"M-64QfW6_pr_","colab_type":"code","colab":{}},"source":["def translate_sentence(sentence, src_field, trg_field1, trg_field2, model, device, max_len = 400):\n","    \n","    model.eval()\n","        \n","    tokens = [token.lower() for token in sentence]\n","\n","    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n","        \n","    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n","\n","    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n","    \n","    src_mask = model.make_src_mask(src_tensor)\n","    \n","    with torch.no_grad():\n","        enc_src = model.encoder(src_tensor, src_mask)\n","\n","    trg_indexes1 = [trg_field1.vocab.stoi[trg_field1.init_token]]\n","    trg_indexes2 = [trg_field2.vocab.stoi[trg_field2.init_token]]\n","\n","    for i in range(max_len):\n","\n","        trg_tensor1 = torch.LongTensor(trg_indexes1).unsqueeze(0).to(device)\n","\n","        trg_mask1 = model.make_trg_mask(trg_tensor1)\n","        \n","        with torch.no_grad():\n","            output1, attention1 = model.decoder1(trg_tensor1, enc_src, trg_mask1, src_mask)\n","        \n","        pred_token1 = output1.argmax(2)[:,-1].item()\n","\n","        trg_indexes1.append(pred_token1)\n","\n","        if pred_token1 == trg_field1.vocab.stoi[trg_field1.eos_token]:\n","            break\n","    \n","    trg_tokens1 = [trg_field1.vocab.itos[i] for i in trg_indexes1]\n","\n","    for i in range(max_len):\n","\n","        trg_tensor2 = torch.LongTensor(trg_indexes2).unsqueeze(0).to(device)\n","\n","        trg_mask2 = model.make_trg_mask(trg_tensor2)\n","        \n","        with torch.no_grad():\n","            output2, attention2 = model.decoder2(trg_tensor2, enc_src, trg_mask2, src_mask)\n","        \n","        pred_token2 = output2.argmax(2)[:,-1].item()\n","\n","        trg_indexes2.append(pred_token2)\n","\n","        if pred_token2 == trg_field2.vocab.stoi[trg_field2.eos_token]:\n","            break\n","    \n","    trg_tokens2 = [trg_field2.vocab.itos[i] for i in trg_indexes2]\n","    \n","    return trg_tokens1[1:], attention1, trg_tokens2[1:], attention2"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-RknxJtQuwOS","colab_type":"text"},"source":["Checking the output for a random index"]},{"cell_type":"code","metadata":{"id":"B7gLrsCa_r-R","colab_type":"code","outputId":"66887197-0556-42ec-8b6b-08bafbb1cc6d","executionInfo":{"status":"ok","timestamp":1591997505760,"user_tz":420,"elapsed":601,"user":{"displayName":"Anurag Sengupta","photoUrl":"","userId":"05018465714817613204"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["example_idx = 25\n","\n","src = vars(tst.examples[example_idx])['English']\n","trg1 = vars(tst.examples[example_idx])['Spanish']\n","trg2 = vars(tst.examples[example_idx])['French']\n","s =\" \"\n","\n","print(f'src = {s.join(src)}')\n","print(f'trg1 = {s.join(trg1)}')\n","print(f'trg2 = {s.join(trg2)}')"],"execution_count":35,"outputs":[{"output_type":"stream","text":["src = i have three very specific questions .\n","trg1 = tengo tres preguntas muy específicas .\n","trg2 = j' ai trois questions très concrètes .\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rWHMchtW_udB","colab_type":"code","outputId":"1fd28639-ff13-47ff-b258-4d67003ade88","executionInfo":{"status":"ok","timestamp":1591997506978,"user_tz":420,"elapsed":1168,"user":{"displayName":"Anurag Sengupta","photoUrl":"","userId":"05018465714817613204"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["translation1, attention1, translation2, attention2 = translate_sentence(src, SRC, TRG1, TRG2, model, device)\n","s = \" \"\n","print(f'predicted trg1 = {s.join(translation1)}')\n","print(f'predicted trg2 = {s.join(translation2)}')"],"execution_count":36,"outputs":[{"output_type":"stream","text":["predicted trg1 = he tres preguntas concretas . <eos>\n","predicted trg2 = j' ai trois questions très spécifiques . <eos>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KDsTPt0Nuyyb","colab_type":"text"},"source":["Calculating the BLEU score"]},{"cell_type":"code","metadata":{"id":"xJqrQ0tP_xfg","colab_type":"code","colab":{}},"source":["def calculate_bleu(data, src_field, trg_field1, trg_field2, model, device, max_len = 400):\n","    count=0\n","    bleu1=0\n","    bleu2=0\n","    for datum in data:\n","        count+=1\n","        src = vars(datum)['English']\n","        trg1 = vars(datum)['Spanish']\n","        trg2 = vars(datum)['French']\n","        \n","        pred_trg1, _, pred_trg2, _ = translate_sentence(src, src_field, trg_field1, trg_field2, model, device, max_len)\n","        \n","        #cut off <eos> token\n","        pred_trg1 = pred_trg1[:-1]\n","        pred_trg2 = pred_trg2[:-1]\n","        bleu1 += sentence_bleu([pred_trg1], trg1)\n","        bleu2 += sentence_bleu([pred_trg2], trg2)\n","        print(count)\n","    return bleu1/count, bleu2/count"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3vX1QD_D_1rn","colab_type":"code","colab":{}},"source":["bleu_score1, bleu_score2 = calculate_bleu(tst, SRC, TRG1, TRG2, model, device)\n","print(f'BLEU score = {bleu_score1*100:.2f}, {bleu_score2*100:.2f}')"],"execution_count":0,"outputs":[]}]}